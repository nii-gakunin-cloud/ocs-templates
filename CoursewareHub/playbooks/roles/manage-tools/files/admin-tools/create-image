#!/bin/bash

SCRIPT_DIR=$(cd $(dirname $0); pwd)

source ~/.hub-config
source $SCRIPT_DIR/functions

usage()
{
  cat <<EOF

Usage:
  ${0##*/} IMAGE_NAME BACKUP_DIR

  Arguments:
    IMAGE_NAME : image name of the jupyter notebook container which creates.
    BACKUP_DIR : directory of backup which stores container image.
EOF
  exit 1
}

reportfailed()
{
    echo "Script failed...exiting. ($*)" 1>&2
    usage
    exit 255
}

humanreadable_to_bytes()
{
    # adapted code from http://stackoverflow.com/questions/4399475/unformat-disk-size-strings/
    (( $BASH_VERSINFO >= 4 )) || reportfailed "requires bash >=4"
    local s="$1"
    tenpower=0
    if [[ "$s" == *.* ]]; then
        local post_decimal="${s#*.}"
        post_decimal="${post_decimal//[^0-9]}"
        tenpower="${#post_decimal}"
    fi
    s=${s^^};s=${s/B};s=${s/.};s=${s/E/KP};s=${s/P/KT}
    s=${s/T/KG};s=${s/G/KM};s=${s/M/KK}

    s=${s//K/*1024}" / ( 10 ** $tenpower )"
    echo $((s))
}

backup_dir="$1"
unique_image_name="server-container-$(TZ=JST-9 date "+%y%m%d-%H%M%S")"

## Do tests that can be done quickly first, so user can fix simple problems faster:
imagefile="$backup_dir/$unique_image_name.tar"
[ -f "$imagefile" ] && reportfailed "The imagefile ($imagename) already exists"
rm -f "$imagefile"
touch "$imagefile" || reportfailed "Unable to open $imagename for writing"

## These tests have some delay
servername="jupyter-$USER"
node_info="$(find_node "$servername")" || exit
IFS=' '
set -- $node_info
thenode=$1
thenode_ip=$2

## get the virtual size for giving % progress output:
echo -n "Starting by getting the image size..."
output=$($SCRIPT_DIR/ssh-node "$thenode_ip" -q sudo docker ps -s -f name="$servername")
echo ".done"
[ "$(wc -l <<<"$output")" == 2 ] || echo "Warning: problem finding image sizes" 1>&2
## output is something like "..........(virtual 4.598 GB)"
tmp1="${output#*virtual }"
bytes="$(humanreadable_to_bytes "${tmp1%)*}")"

## (Note: An earlier version of this code tried to use "docker
## export" of a *container* here.  It failed because export strips
## out metadata from the image that is required by jupyterhub.
## Therefore, the code here now uses "docker save" of an *image*,
## and therefore commit is necessary to create an up-to-date
## image. )

echo "Making new commit with name $unique_image_name..."
$SCRIPT_DIR/ssh-node "$thenode_ip" -q sudo docker commit "$servername" "$unique_image_name" \
   || reportfailed "Commit of $servername"
echo "..Finished commit."

echo "Starting save image. Please wait for several minutes..."
$SCRIPT_DIR/ssh-node "$thenode_ip" -q sudo docker save "$unique_image_name" >"$imagefile.inprogress" &
pid="$!"
    
echo "0 / $bytes  (0%)"
while [ -d "/proc/$pid" ]; do
    read perms links owner group size theres <<<"$(ls -l "$imagefile.inprogress")"
    if [ "$size" != "0" ]; then
        echo "$size / $bytes  ($((size * 100 / bytes ))%)"
    fi
    sleep 15
done
wait "$pid"
[ "$?" = "0" ] || reportfailed "docker save"
mv "$imagefile.inprogress" "$imagefile" || reportfailed "Final renaming to $imagename"
echo "...finished docker save for $imagename."
echo

